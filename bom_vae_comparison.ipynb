{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOM-VAE vs \u03b2-VAE Comparison\n",
    "\n",
    "**Hypothesis**: BOM achieves comparable or better results than \u03b2-VAE without requiring hyperparameter tuning.\n",
    "\n",
    "**Adaptive squeeze rule**:\n",
    "```\n",
    "squeeze_amount = (s_min - 0.5) * k\n",
    "```\n",
    "- When s_min = 0.9: squeeze aggressively\n",
    "- When s_min = 0.55: squeeze gently  \n",
    "- When s_min \u2264 0.5: stop squeezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared VAE architecture\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 2, 1), nn.GroupNorm(8, 32), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.GroupNorm(8, 64), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1), nn.GroupNorm(8, 128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1), nn.GroupNorm(8, 256), nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256*4*4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256*4*4, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, 256*4*4)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, 2, 1, 1), nn.GroupNorm(8, 128), nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, 3, 2, 1, 1), nn.GroupNorm(8, 64), nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 32, 3, 2, 1, 1), nn.GroupNorm(8, 32), nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 1, 3, 2, 1, 1), nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.enc(x).view(x.size(0), -1)\n",
    "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
    "        z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n",
    "        return self.dec(self.fc_dec(z).view(-1, 256, 4, 4)), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data\ntransform = transforms.Compose([transforms.Resize(64), transforms.ToTensor()])\ntrain_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\ntest_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=2)\nprint(f\"Train: {len(train_loader)} batches, Test: {len(test_loader)} batches\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_index(x, y, c1=0.01**2, c2=0.03**2):\n",
    "    \"\"\"Compute per-image SSIM. Expects x,y in [0,1].\"\"\"\n",
    "    mu_x = x.mean([2, 3], keepdim=True)\n",
    "    mu_y = y.mean([2, 3], keepdim=True)\n",
    "    sigma_x = ((x - mu_x) ** 2).mean([2, 3], keepdim=True)\n",
    "    sigma_y = ((y - mu_y) ** 2).mean([2, 3], keepdim=True)\n",
    "    sigma_xy = ((x - mu_x) * (y - mu_y)).mean([2, 3], keepdim=True)\n",
    "\n",
    "    numerator = (2 * mu_x * mu_y + c1) * (2 * sigma_xy + c2)\n",
    "    denominator = (mu_x ** 2 + mu_y ** 2 + c1) * (sigma_x + sigma_y + c2)\n",
    "    ssim_map = numerator / (denominator + 1e-8)\n",
    "    return ssim_map.mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def compute_metrics(x, x_recon, mu, logvar):\n",
    "    \"\"\"Compute MSE, KL, sharpness, and SSIM.\"\"\"\n",
    "    B = x.size(0)\n",
    "    mse = F.mse_loss(x_recon, x, reduction='none').view(B, -1).mean(1)\n",
    "    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).sum(1)\n",
    "    dx = torch.abs(x_recon[:, :, :, 1:] - x_recon[:, :, :, :-1])\n",
    "    dy = torch.abs(x_recon[:, :, 1:, :] - x_recon[:, :, :-1, :])\n",
    "    sharp = (dx.mean([1, 2, 3]) + dy.mean([1, 2, 3])) / 2\n",
    "    ssim = ssim_index(x, x_recon)\n",
    "    return mse, kl, sharp, ssim\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    all_mse, all_kl, all_sharp, all_ssim = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0].to(device)\n",
    "            x_recon, mu, logvar = model(x)\n",
    "            mse, kl, sharp, ssim = compute_metrics(x, x_recon, mu, logvar)\n",
    "            all_mse.extend(mse.cpu().numpy())\n",
    "            all_kl.extend(kl.cpu().numpy())\n",
    "            all_sharp.extend(sharp.cpu().numpy())\n",
    "            all_ssim.extend(ssim.cpu().numpy())\n",
    "    \n",
    "    return {\n",
    "        'mse': np.mean(all_mse),\n",
    "        'kl': np.mean(all_kl),\n",
    "        'sharp': np.mean(all_sharp),\n",
    "        'ssim': np.mean(all_ssim),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \u03b2-VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_beta_vae(model, loader, device, beta, n_epochs=20):\n",
    "    \"\"\"\n",
    "    Standard \u03b2-VAE training.\n",
    "    Loss = MSE + \u03b2 * KL\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss, epoch_mse, epoch_kl = [], [], []\n",
    "        \n",
    "        pbar = tqdm(loader, desc=f\"\u03b2-VAE (\u03b2={beta}) Epoch {epoch}\")\n",
    "        for batch in pbar:\n",
    "            x = batch[0].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            x_recon, mu, logvar = model(x)\n",
    "            \n",
    "            mse, kl, sharp, ssim = compute_metrics(x, x_recon, mu, logvar)\n",
    "            loss = mse.mean() + beta * kl.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_mse.append(mse.mean().item())\n",
    "            epoch_kl.append(kl.mean().item())\n",
    "            \n",
    "            history.append({\n",
    "                'mse': mse.mean().item(),\n",
    "                'kl': kl.mean().item(),\n",
    "                'sharp': sharp.mean().item(),\n",
    "                'ssim': ssim.mean().item(),\n",
    "            })\n",
    "            \n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'mse': f\"{mse.mean().item():.4f}\", 'kl': f\"{kl.mean().item():.0f}\"})\n",
    "        \n",
    "        print(f\"  Epoch {epoch}: loss={np.mean(epoch_loss):.4f}, mse={np.mean(epoch_mse):.4f}, kl={np.mean(epoch_kl):.0f}\")\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BOM-VAE Training with Adaptive Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_constraint_lower_better(value, floor):\n",
    "    \"\"\"Score for objectives where lower is better (MSE).\"\"\"\n",
    "    return (floor - value) / floor\n",
    "\n",
    "\n",
    "def regular_constraint_higher_better(value, ceiling):\n",
    "    \"\"\"Score for objectives where higher is better (SSIM).\"\"\"\n",
    "    return value / ceiling\n",
    "\n",
    "\n",
    "def box_constraint(value, floor_low, optimum, floor_high):\n",
    "    \"\"\"Score for objectives that need to stay in a range (KL).\"\"\"\n",
    "    left = (value - floor_low) / (optimum - floor_low)\n",
    "    right = (floor_high - value) / (floor_high - optimum)\n",
    "    return torch.minimum(left, right)\n",
    "\n",
    "\n",
    "def compute_bom_loss(\n",
    "    x, x_recon, mu, logvar,\n",
    "    mse_floor, kl_floor_low, kl_optimum, kl_floor_high, ssim_ceiling\n",
    "): \n",
    "    \"\"\"Compute BOM loss using MSE, KL, and SSIM.\"\"\"\n",
    "    mse, kl, sharp, ssim = compute_metrics(x, x_recon, mu, logvar)\n",
    "    \n",
    "    mse_score = regular_constraint_lower_better(mse, mse_floor)\n",
    "    kl_score = box_constraint(kl, kl_floor_low, kl_optimum, kl_floor_high)\n",
    "    ssim_score = regular_constraint_higher_better(ssim, ssim_ceiling)\n",
    "    \n",
    "    scores = torch.stack([mse_score, kl_score, ssim_score], dim=1)\n",
    "    s_min, min_idx = torch.min(scores, dim=1)\n",
    "    \n",
    "    violations = (s_min <= 0).sum().item()\n",
    "    \n",
    "    metrics = {\n",
    "        'mse': mse.mean().item(),\n",
    "        'kl': kl.mean().item(),\n",
    "        'sharp': sharp.mean().item(),\n",
    "        'ssim': ssim.mean().item(),\n",
    "        'mse_score': mse_score.mean().item(),\n",
    "        'kl_score': kl_score.mean().item(),\n",
    "        'ssim_score': ssim_score.mean().item(),\n",
    "        's_min': s_min.mean().item(),\n",
    "        'violations': violations,\n",
    "    }\n",
    "    \n",
    "    if violations > 0:\n",
    "        return None, metrics\n",
    "    \n",
    "    loss = -torch.log(s_min).mean()\n",
    "    names = ['mse', 'kl', 'ssim']\n",
    "    metrics['bottleneck'] = names[torch.bincount(min_idx, minlength=3).argmax().item()]\n",
    "    metrics['loss'] = loss.item()\n",
    "    \n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_bom(model, loader, device, n_batches=50):\n",
    "    \"\"\"Calibrate BOM constraints based on model's current outputs.\"\"\"\n",
    "    model.train()  # Important: use train mode for BatchNorm\n",
    "    all_mse, all_kl, all_sharp, all_ssim = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= n_batches: break\n",
    "            x = batch[0].to(device)\n",
    "            x_recon, mu, logvar = model(x)\n",
    "            mse, kl, sharp, ssim = compute_metrics(x, x_recon, mu, logvar)\n",
    "            all_mse.extend(mse.cpu().numpy())\n",
    "            all_kl.extend(kl.cpu().numpy())\n",
    "            all_sharp.extend(sharp.cpu().numpy())\n",
    "            all_ssim.extend(ssim.cpu().numpy())\n",
    "    \n",
    "    mse_arr = np.array(all_mse)\n",
    "    kl_arr = np.array(all_kl)\n",
    "    sharp_arr = np.array(all_sharp)\n",
    "    ssim_arr = np.array(all_ssim)\n",
    "    \n",
    "    ssim_ceiling = min(0.95, ssim_arr.mean() + 0.05)\n",
    "    \n",
    "    params = {\n",
    "        'mse_floor': mse_arr.max() * 2.0,\n",
    "        'kl_floor_low': kl_arr.min() * 0.1,\n",
    "        'kl_optimum': kl_arr.mean(),\n",
    "        'kl_floor_high': kl_arr.max() * 50.0,  # Very loose initially\n",
    "        'ssim_ceiling': ssim_ceiling,\n",
    "    }\n",
    "    \n",
    "    print(f\"Calibration: MSE={mse_arr.mean():.4f}, KL={kl_arr.mean():.1f}, Sharp={sharp_arr.mean():.4f}, SSIM={ssim_arr.mean():.4f}\")\n",
    "    print(\n",
    "        f\"Initial constraints: mse_floor={params['mse_floor']:.4f}, \"\n",
    "        f\"kl_box=[{params['kl_floor_low']:.1f}, {params['kl_optimum']:.1f}, {params['kl_floor_high']:.1f}], \"\n",
    "        f\"ssim_ceiling={params['ssim_ceiling']:.3f}\"\n",
    "    )\n",
    "    \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bom_vae(model, loader, device, n_epochs=20):\n",
    "    \"\"\"\n",
    "    BOM-VAE training with adaptive squeeze.\n",
    "    \n",
    "    Squeeze rule: squeeze_amount = (s_min - 0.5) * k\n",
    "    - s_min > 0.5: squeeze proportionally\n",
    "    - s_min <= 0.5: don't squeeze\n",
    "    \"\"\"\n",
    "    # Calibrate\n",
    "    params = calibrate_bom(model, loader, device)\n",
    "    \n",
    "    mse_floor = params['mse_floor']\n",
    "    kl_floor_low = params['kl_floor_low']\n",
    "    kl_optimum = params['kl_optimum']\n",
    "    kl_floor_high = params['kl_floor_high']\n",
    "    ssim_ceiling = params['ssim_ceiling']\n",
    "    \n",
    "    # Targets\n",
    "    target_kl_floor_low = 50\n",
    "    target_kl_optimum = 80\n",
    "    target_kl_floor_high = 150\n",
    "    \n",
    "    # Adaptive squeeze settings\n",
    "    squeeze_k = 0.5  # Gain factor\n",
    "    min_s_min_for_squeeze = 0.5\n",
    "    squeeze_start_epoch = 3\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss, epoch_s_min = [], []\n",
    "        epoch_violations = 0\n",
    "        \n",
    "        pbar = tqdm(loader, desc=f\"BOM-VAE Epoch {epoch}\")\n",
    "        for batch in pbar:\n",
    "            x = batch[0].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            x_recon, mu, logvar = model(x)\n",
    "            \n",
    "            loss, metrics = compute_bom_loss(\n",
    "                x, x_recon, mu, logvar,\n",
    "                mse_floor, kl_floor_low, kl_optimum, kl_floor_high, ssim_ceiling\n",
    "            )\n",
    "            \n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                epoch_loss.append(metrics['loss'])\n",
    "                epoch_s_min.append(metrics['s_min'])\n",
    "            else:\n",
    "                epoch_violations += metrics['violations']\n",
    "            \n",
    "            history.append(metrics)\n",
    "            pbar.set_postfix({'s_min': f\"{metrics['s_min']:.3f}\", 'kl': f\"{metrics['kl']:.0f}\"})\n",
    "        \n",
    "        avg_s_min = np.mean(epoch_s_min) if epoch_s_min else 0\n",
    "        print(\n",
    "            f\"  Epoch {epoch}: s_min={avg_s_min:.3f}, violations={epoch_violations}, \"\n",
    "            f\"mse={metrics['mse']:.4f}, kl={metrics['kl']:.0f}, ssim={metrics['ssim']:.3f}\"\n",
    "        )\n",
    "        print(f\"    KL box: [{kl_floor_low:.1f}, {kl_optimum:.1f}, {kl_floor_high:.1f}]\")\n",
    "        print(f\"    SSIM ceiling: {ssim_ceiling:.3f}\")\n",
    "        \n",
    "        # Adaptive squeeze\n",
    "        if epoch >= squeeze_start_epoch and avg_s_min > min_s_min_for_squeeze:\n",
    "            squeeze_amount = (avg_s_min - min_s_min_for_squeeze) * squeeze_k\n",
    "            squeeze_factor = 1.0 - squeeze_amount  # e.g., s_min=0.9 -> factor=0.8\n",
    "            squeeze_factor = max(0.5, squeeze_factor)  # Don't squeeze more than 50%\n",
    "            \n",
    "            print(f\"    \ud83d\udd27 Squeeze: s_min={avg_s_min:.3f} -> factor={squeeze_factor:.2f}\")\n",
    "            \n",
    "            # Squeeze MSE floor\n",
    "            mse_floor *= squeeze_factor\n",
    "            \n",
    "            # Squeeze KL box toward targets\n",
    "            if kl_floor_low < target_kl_floor_low:\n",
    "                kl_floor_low += (target_kl_floor_low - kl_floor_low) * (1 - squeeze_factor)\n",
    "            if kl_optimum < target_kl_optimum:\n",
    "                kl_optimum += (target_kl_optimum - kl_optimum) * (1 - squeeze_factor)\n",
    "            if kl_floor_high > target_kl_floor_high:\n",
    "                kl_floor_high -= (kl_floor_high - target_kl_floor_high) * (1 - squeeze_factor)\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u03b2-VAE with different \u03b2 values\n",
    "betas = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "for beta in betas:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training \u03b2-VAE with \u03b2={beta}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    model = VAE(latent_dim=128).to(device)\n",
    "    history = train_beta_vae(model, train_loader, device, beta=beta, n_epochs=N_EPOCHS)\n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "    \n",
    "    results[f'beta_{beta}'] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test': test_metrics,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTest results: MSE={test_metrics['mse']:.4f}, KL={test_metrics['kl']:.1f}, SSIM={test_metrics['ssim']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOM-VAE\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training BOM-VAE (no \u03b2 tuning required)\")\n",
    "print('='*60)\n",
    "\n",
    "model_bom = VAE(latent_dim=128).to(device)\n",
    "history_bom = train_bom_vae(model_bom, train_loader, device, n_epochs=N_EPOCHS)\n",
    "test_metrics_bom = evaluate(model_bom, test_loader, device)\n",
    "\n",
    "results['bom'] = {\n",
    "    'model': model_bom,\n",
    "    'history': history_bom,\n",
    "    'test': test_metrics_bom,\n",
    "}\n",
    "\n",
    "print(f\"\\nTest results: MSE={test_metrics_bom['mse']:.4f}, KL={test_metrics_bom['kl']:.1f}, SSIM={test_metrics_bom['ssim']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<20} {'MSE':>10} {'KL':>10} {'SSIM':>10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for name, data in results.items():\n",
    "    t = data['test']\n",
    "    print(f\"{name:<20} {t['mse']:>10.4f} {t['kl']:>10.1f} {t['ssim']:>10.4f}\")\n",
    "\n",
    "print(\"-\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for name, data in results.items():\n",
    "    h = data['history']\n",
    "    label = name.replace('_', '=')\n",
    "    \n",
    "    axes[0].plot([x['mse'] for x in h], label=label, alpha=0.8)\n",
    "    axes[1].plot([x['kl'] for x in h], label=label, alpha=0.8)\n",
    "    axes[2].plot([x['ssim'] for x in h], label=label, alpha=0.8)\n",
    "\n",
    "axes[0].set_title('MSE (\u2193 better)')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].set_title('KL Divergence')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].set_title('SSIM (\u2191 better)')\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_comparison.png', dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto plot: MSE vs KL\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(results)))\n",
    "\n",
    "for (name, data), color in zip(results.items(), colors):\n",
    "    t = data['test']\n",
    "    marker = 's' if 'beta' in name else 'o'\n",
    "    size = 100 if 'bom' in name else 60\n",
    "    plt.scatter(t['mse'], t['kl'], s=size, c=[color], marker=marker, label=name.replace('_', '='), edgecolors='black')\n",
    "\n",
    "plt.xlabel('MSE (\u2193 better)')\n",
    "plt.ylabel('KL (moderate is better)')\n",
    "plt.title('Pareto Front: MSE vs KL')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('pareto_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reconstructions comparison\ntest_batch = next(iter(test_loader))[0][:8].to(device)\n\nn_models = len(results)\nfig, axes = plt.subplots(n_models + 1, 8, figsize=(16, 2*(n_models+1)))\n\n# Original\nfor i in range(8):\n    axes[0, i].imshow(test_batch[i].cpu().squeeze(0))\n    axes[0, i].axis('off')\naxes[0, 0].set_ylabel('Original', fontsize=12)\n\n# Each model's reconstruction\nfor row, (name, data) in enumerate(results.items(), 1):\n    model = data['model']\n    model.eval()\n    with torch.no_grad():\n        recon, _, _ = model(test_batch)\n    \n    for i in range(8):\n        axes[row, i].imshow(recon[i].cpu().squeeze(0))\n        axes[row, i].axis('off')\n    axes[row, 0].set_ylabel(name.replace('_', '='), fontsize=12)\n\nplt.tight_layout()\nplt.savefig('reconstructions_comparison.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Samples from prior comparison\nz = torch.randn(8, 128, device=device)\n\nn_models = len(results)\nfig, axes = plt.subplots(n_models, 8, figsize=(16, 2*n_models))\n\nfor row, (name, data) in enumerate(results.items()):\n    model = data['model']\n    model.eval()\n    with torch.no_grad():\n        samples = model.dec(model.fc_dec(z).view(-1, 256, 4, 4))\n    \n    for i in range(8):\n        axes[row, i].imshow(samples[i].cpu().squeeze(0))\n        axes[row, i].axis('off')\n    axes[row, 0].set_ylabel(name.replace('_', '='), fontsize=12)\n\nplt.suptitle('Samples from Prior (same z for all models)', fontsize=14)\nplt.tight_layout()\nplt.savefig('samples_comparison.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best \u03b2-VAE\n",
    "beta_results = {k: v for k, v in results.items() if 'beta' in k}\n",
    "bom_result = results['bom']\n",
    "\n",
    "# Best by MSE\n",
    "best_mse_beta = min(beta_results.items(), key=lambda x: x[1]['test']['mse'])\n",
    "print(f\"Best \u03b2-VAE by MSE: {best_mse_beta[0]} (MSE={best_mse_beta[1]['test']['mse']:.4f})\")\n",
    "print(f\"BOM-VAE MSE: {bom_result['test']['mse']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Best by balanced score (low MSE, moderate KL, high SSIM)\n",
    "def balanced_score(t):\n",
    "    # Lower MSE is better (invert)\n",
    "    # KL around 50-150 is good (penalty for too low or too high)\n",
    "    # Higher SSIM is better\n",
    "    mse_score = 1.0 / (t['mse'] + 0.001)\n",
    "    kl_score = 1.0 / (abs(t['kl'] - 100) + 10)  # Peak at KL=100\n",
    "    ssim_score = t['ssim']\n",
    "    return mse_score * kl_score * ssim_score\n",
    "\n",
    "best_balanced_beta = max(beta_results.items(), key=lambda x: balanced_score(x[1]['test']))\n",
    "print(f\"Best \u03b2-VAE by balanced score: {best_balanced_beta[0]}\")\n",
    "print(f\"  Score: {balanced_score(best_balanced_beta[1]['test']):.4f}\")\n",
    "print(f\"BOM-VAE balanced score: {balanced_score(bom_result['test']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "\u03b2-VAE requires tuning \u03b2 to balance reconstruction vs regularization.\n",
    "Different \u03b2 values give different tradeoffs:\n",
    "- Low \u03b2 (0.0001): Good MSE, but KL may collapse or explode\n",
    "- High \u03b2 (0.1): Controlled KL, but poor reconstruction\n",
    "\n",
    "BOM-VAE automatically finds a balanced solution:\n",
    "- No \u03b2 hyperparameter to tune\n",
    "- Adaptive squeeze finds the Pareto frontier\n",
    "- All objectives are explicitly constrained\n",
    "\n",
    "Using SSIM alongside MSE makes it easier to line up scores with\n",
    "visual similarity, since MSE can look good numerically while\n",
    "still missing fine details.\n",
    "\n",
    "The key insight: BOM optimizes the WORST objective at each step,\n",
    "preventing any single objective from being sacrificed.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experimental: BHiLBO-VAE (BHiVAE + LBO)\n",
    "\n",
    "This section adds a minimal BHiLBO-VAE implementation and a quick smoke-test training loop.\n",
    "It is intended for rapid sanity checks (not a full benchmark).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.distributions import MultivariateNormal, kl_divergence\n",
    "\n",
    "class BHiLBO_VAE(nn.Module):\n",
    "    def __init__(self, input_dim=64 * 64, hidden_dim=400, core_latent_dim=16, mid_latent_dim=32, detail_latent_dim=64, block_size=4):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.core_latent_dim = core_latent_dim\n",
    "        self.mid_latent_dim = mid_latent_dim\n",
    "        self.detail_latent_dim = detail_latent_dim\n",
    "\n",
    "        # Hierarchical encoder\n",
    "        self.fc_shared = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mid_hidden = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc_detail_hidden = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "\n",
    "        self.fc_mu_core = nn.Linear(hidden_dim, core_latent_dim)\n",
    "        self.fc_logvar_core = nn.Linear(hidden_dim, core_latent_dim)\n",
    "\n",
    "        self.fc_mu_mid = nn.Linear(hidden_dim // 2, mid_latent_dim)\n",
    "        self.fc_logvar_mid = nn.Linear(hidden_dim // 2, mid_latent_dim)\n",
    "\n",
    "        self.fc_mu_detail = nn.Linear(hidden_dim // 4, detail_latent_dim)\n",
    "        self.fc_logvar_detail = nn.Linear(hidden_dim // 4, detail_latent_dim)\n",
    "\n",
    "        total_latent_dim = core_latent_dim + mid_latent_dim + detail_latent_dim\n",
    "        self.total_latent_dim = total_latent_dim\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decode_hidden = nn.Linear(total_latent_dim, hidden_dim)\n",
    "        self.fc_decode_out = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        # Discriminator (for recon realism)\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        # TC separator (density ratio)\n",
    "        self.d_sep = nn.Sequential(\n",
    "            nn.Linear(total_latent_dim + hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Block-diagonal prior\n",
    "        prior_cov = torch.eye(total_latent_dim)\n",
    "        for start in range(0, total_latent_dim, block_size):\n",
    "            end = min(start + block_size, total_latent_dim)\n",
    "            block = torch.full((end - start, end - start), 0.5)\n",
    "            block.diagonal().fill_(1.0)\n",
    "            prior_cov[start:end, start:end] = block\n",
    "        self.register_buffer('prior_cov', prior_cov)\n",
    "        self.register_buffer('prior_mean', torch.zeros(total_latent_dim))\n",
    "\n",
    "        # Constraint configs (minimal set for smoke test)\n",
    "        self.constraints = {\n",
    "            'recon': {'type': 'BOX', 'target': 0.0, 'failure': 0.12},\n",
    "            'kl_core': {'type': 'BOX', 'target': 100.0, 'failure': 4000.0},\n",
    "            'kl_mid': {'type': 'BOX', 'target': 100.0, 'failure': 4000.0},\n",
    "            'kl_detail': {'type': 'BOX', 'target': 100.0, 'failure': 4000.0},\n",
    "            'disc': {'type': 'BOX', 'target': 0.5, 'failure': 0.0},\n",
    "            'sep': {'type': 'BOX', 'target': 0.0, 'failure': 50.0},\n",
    "            'prior': {'type': 'BOX', 'target': 1.0, 'failure': 100.0},\n",
    "        }\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc_shared(x))\n",
    "        mu_core = self.fc_mu_core(h1)\n",
    "        logvar_core = self.fc_logvar_core(h1)\n",
    "\n",
    "        h2 = F.relu(self.fc_mid_hidden(h1))\n",
    "        mu_mid = self.fc_mu_mid(h2)\n",
    "        logvar_mid = self.fc_logvar_mid(h2)\n",
    "\n",
    "        h3 = F.relu(self.fc_detail_hidden(h2))\n",
    "        mu_detail = self.fc_mu_detail(h3)\n",
    "        logvar_detail = self.fc_logvar_detail(h3)\n",
    "\n",
    "        return (mu_core, mu_mid, mu_detail), (logvar_core, logvar_mid, logvar_detail), (h1, h2, h3)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc_decode_hidden(z))\n",
    "        return torch.sigmoid(self.fc_decode_out(h))\n",
    "\n",
    "    def estimate_tc(self, z, h):\n",
    "        z_i = torch.cat([z, h], dim=1)\n",
    "        perm = torch.randperm(z_i.size(0))\n",
    "        z_perm = torch.cat([z[perm], h], dim=1)\n",
    "        d_true = self.d_sep(z_i)\n",
    "        d_perm = self.d_sep(z_perm)\n",
    "        return torch.log(d_true / (1 - d_true + 1e-6)).mean()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        mus, logvars, hs = self.encode(x_flat)\n",
    "        mu_core, mu_mid, mu_detail = mus\n",
    "        logvar_core, logvar_mid, logvar_detail = logvars\n",
    "        h1, h2, h3 = hs\n",
    "\n",
    "        z_core = self.reparameterize(mu_core, logvar_core)\n",
    "        z_mid = self.reparameterize(mu_mid, logvar_mid)\n",
    "        z_detail = self.reparameterize(mu_detail, logvar_detail)\n",
    "\n",
    "        z = torch.cat([z_core, z_mid, z_detail], dim=1)\n",
    "        recon_x = self.decode(z)\n",
    "\n",
    "        raw_recon = F.binary_cross_entropy(recon_x, x_flat, reduction='none').mean(dim=1)\n",
    "        raw_kl_core = -0.5 * torch.sum(1 + logvar_core - mu_core.pow(2) - logvar_core.exp(), dim=1)\n",
    "        raw_kl_mid = -0.5 * torch.sum(1 + logvar_mid - mu_mid.pow(2) - logvar_mid.exp(), dim=1)\n",
    "        raw_kl_detail = -0.5 * torch.sum(1 + logvar_detail - mu_detail.pow(2) - logvar_detail.exp(), dim=1)\n",
    "        raw_disc = torch.sigmoid(self.disc(recon_x)).squeeze(-1)\n",
    "\n",
    "        raw_sep = self.estimate_tc(z, h1)\n",
    "\n",
    "        q_dist = MultivariateNormal(torch.cat(mus, dim=1), torch.diag_embed(torch.cat(logvars, dim=1).exp()))\n",
    "        p_dist = MultivariateNormal(self.prior_mean, self.prior_cov)\n",
    "        raw_prior = kl_divergence(q_dist, p_dist)\n",
    "\n",
    "        metrics = {\n",
    "            'recon': raw_recon,\n",
    "            'kl_core': raw_kl_core,\n",
    "            'kl_mid': raw_kl_mid,\n",
    "            'kl_detail': raw_kl_detail,\n",
    "            'disc': raw_disc,\n",
    "            'sep': raw_sep,\n",
    "            'prior': raw_prior\n",
    "        }\n",
    "        return recon_x, metrics\n",
    "\n",
    "def normalize_scores(metrics, cfg):\n",
    "    S = {}\n",
    "    S['recon'] = (cfg['recon']['failure'] - metrics['recon']) / (cfg['recon']['failure'] - cfg['recon']['target'])\n",
    "    for k in ['kl_core', 'kl_mid', 'kl_detail']:\n",
    "        S[k] = (cfg[k]['failure'] - metrics[k]) / (cfg[k]['failure'] - cfg[k]['target'])\n",
    "    S['disc'] = (metrics['disc'] - cfg['disc']['failure']) / (cfg['disc']['target'] - cfg['disc']['failure'])\n",
    "    S['sep'] = (cfg['sep']['failure'] - metrics['sep']) / (cfg['sep']['failure'] - cfg['sep']['target'])\n",
    "    S['prior'] = (cfg['prior']['failure'] - metrics['prior']) / (cfg['prior']['failure'] - cfg['prior']['target'])\n",
    "    return S\n",
    "\n",
    "def lbo_loss_from_scores(S):\n",
    "    stacked = torch.stack(list(S.values()), dim=1)\n",
    "    return stacked.min(dim=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Smoke-test: small subset + few batches to confirm plumbing\n",
    "small_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2)\n",
    "model_bhilbo = BHiLBO_VAE().to(device)\n",
    "optim_main = optim.Adam(model_bhilbo.parameters(), lr=2e-3)\n",
    "optim_disc = optim.Adam(list(model_bhilbo.disc.parameters()) + list(model_bhilbo.d_sep.parameters()), lr=2e-4)\n",
    "\n",
    "model_bhilbo.train()\n",
    "for i, (x, _) in enumerate(small_loader):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    x = x.to(device)\n",
    "\n",
    "    # Train discriminators\n",
    "    recon_x, _ = model_bhilbo(x)\n",
    "    optim_disc.zero_grad()\n",
    "    d_real = model_bhilbo.disc(x.view(x.size(0), -1))\n",
    "    d_fake = model_bhilbo.disc(recon_x.detach())\n",
    "    loss_disc = (\n",
    "        F.binary_cross_entropy_with_logits(d_real, torch.ones_like(d_real))\n",
    "        + F.binary_cross_entropy_with_logits(d_fake, torch.zeros_like(d_fake))\n",
    "    )\n",
    "    loss_disc.backward()\n",
    "    optim_disc.step()\n",
    "\n",
    "    # Main LBO step\n",
    "    optim_main.zero_grad()\n",
    "    recon_x, metrics = model_bhilbo(x)\n",
    "    S = normalize_scores(metrics, model_bhilbo.constraints)\n",
    "    min_s = lbo_loss_from_scores(S)\n",
    "    if (min_s <= 0).any() or torch.isnan(min_s).any():\n",
    "        continue\n",
    "    loss = -torch.log(min_s).mean()\n",
    "    loss.backward()\n",
    "    optim_main.step()\n",
    "\n",
    "print('BHiLBO-VAE smoke test complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experimental: Log-Sum Goal VAE (log goals gradient scaling)\n",
    "\n",
    "This variant uses the same metrics but optimizes the sum of log(goal) values,\n",
    "so each goal receives a gradient scaled by 1/goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def logsum_goal_loss(S):\n",
    "    \"\"\"Return per-sample loss = -sum(log(goal)).\n",
    "    Each goal's gradient is scaled by 1/goal.\"\"\"\n",
    "    stacked = torch.stack(list(S.values()), dim=1)\n",
    "    return -(torch.log(stacked).sum(dim=1))\n",
    "\n",
    "# Smoke-test: log-sum goal loss variant\n",
    "model_logsum = BHiLBO_VAE().to(device)\n",
    "optim_logsum = optim.Adam(model_logsum.parameters(), lr=2e-3)\n",
    "\n",
    "model_logsum.train()\n",
    "for i, (x, _) in enumerate(small_loader):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    x = x.to(device)\n",
    "    optim_logsum.zero_grad()\n",
    "    recon_x, metrics = model_logsum(x)\n",
    "    S = normalize_scores(metrics, model_logsum.constraints)\n",
    "    min_s = lbo_loss_from_scores(S)\n",
    "    if (min_s <= 0).any() or torch.isnan(min_s).any():\n",
    "        continue\n",
    "    loss = logsum_goal_loss(S).mean()\n",
    "    loss.backward()\n",
    "    optim_logsum.step()\n",
    "\n",
    "print('Log-sum goal VAE smoke test complete.')\n"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 }
}