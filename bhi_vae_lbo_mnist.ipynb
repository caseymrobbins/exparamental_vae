{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BHiVAE with LBO Bottleneck (MNIST)\n",
    "\n",
    "This notebook remakes the training flow from scratch using a BHiVAE architecture with an LBO-style bottleneck and the **Muon** optimizer instead of Adam.\n",
    "\n",
    "## Colab quick start\n",
    "If you're in Colab, run the next cell to clone the repo and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Colab) Clone repo and install dependencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    repo_url = 'https://github.com/<your-org>/<your-repo>.git'  # TODO: replace\n",
    "    repo_dir = Path('/content/exparamental_vae')\n",
    "    if not repo_dir.exists():\n",
    "        !git clone {repo_url} {repo_dir}\n",
    "    %cd {repo_dir}\n",
    "    !pip -q install torch torchvision tqdm matplotlib\n",
    "else:\n",
    "    print('Not running in Colab; skipping clone/install.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'image_size': 28,\n",
    "    'latent_dim': 32,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 30,\n",
    "    'lr': 2e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'decoder_likelihood': 'bernoulli',  # 'bernoulli' -> BCE, 'gaussian' -> MSE\n",
    "    # LBO bottleneck targets\n",
    "    'D_ok': 0.12,\n",
    "    'K_min': 6.0,\n",
    "    'K_max': 18.0,\n",
    "    'tau_D': 0.03,\n",
    "    'tau_K_low': 2.0,\n",
    "    'tau_K_high': 2.0,\n",
    "    'tau_M': 40.0,\n",
    "    'B_max': 0.08,\n",
    "    'tau_B': 0.02,\n",
    "    'log_every': 100,\n",
    "    'sample_every': 1,\n",
    "    'sample_dir': 'samples',\n",
    "    'num_samples': 8,\n",
    "}\n",
    "config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (BHiVAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BHiVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 2, 1), nn.GroupNorm(8, 32), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.GroupNorm(8, 64), nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.GroupNorm(8, 32), nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1), nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc(x).view(x.size(0), -1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_dec(z).view(-1, 64, 7, 7)\n",
    "        return self.dec(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muon Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"Minimal Muon optimizer (momentum + normalized update direction).\n",
    "\n",
    "    This is a light-weight placeholder implementation. Adjust per your Muon spec.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=2e-4, momentum=0.95, weight_decay=1e-4):\n",
    "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    grad = grad.add(p, alpha=weight_decay)\n",
    "\n",
    "                state = self.state[p]\n",
    "                if 'velocity' not in state:\n",
    "                    state['velocity'] = torch.zeros_like(p)\n",
    "\n",
    "                v = state['velocity']\n",
    "                v.mul_(momentum).add_(grad)\n",
    "\n",
    "                denom = v.norm().clamp_min(1e-8)\n",
    "                step = v / denom\n",
    "                p.add_(step, alpha=-lr)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config['image_size']),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "print(f'Train batches: {len(train_loader)}, Test batches: {len(test_loader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBO Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu, logvar):\n",
    "    return -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).sum(1)\n",
    "\n",
    "def recon_loss(x, x_hat, likelihood='bernoulli'):\n",
    "    if likelihood == 'bernoulli':\n",
    "        return F.binary_cross_entropy(x_hat, x, reduction='none').view(x.size(0), -1).mean(1)\n",
    "    return F.mse_loss(x_hat, x, reduction='none').view(x.size(0), -1).mean(1)\n",
    "\n",
    "def bottleneck_gates(x, x_hat, mu, logvar, cfg):\n",
    "    D = recon_loss(x, x_hat, cfg['decoder_likelihood'])\n",
    "    K = kl_divergence(mu, logvar)\n",
    "\n",
    "    g_recon = torch.exp(-(torch.relu(D - cfg['D_ok']) / cfg['tau_D']))\n",
    "    g_kl_low = torch.exp(-(torch.relu(cfg['K_min'] - K) / cfg['tau_K_low']))\n",
    "    g_kl_high = torch.exp(-(torch.relu(K - cfg['K_max']) / cfg['tau_K_high']))\n",
    "\n",
    "    ink_x = x.view(x.size(0), -1).sum(1)\n",
    "    ink_hat = x_hat.view(x_hat.size(0), -1).sum(1)\n",
    "    g_ink = torch.exp(-(torch.abs(ink_hat - ink_x) / cfg['tau_M']))\n",
    "\n",
    "    bg = x_hat.mean(dim=[1, 2, 3])\n",
    "    g_bg = torch.exp(-(torch.relu(bg - cfg['B_max']) / cfg['tau_B']))\n",
    "\n",
    "    gates = {\n",
    "        'g_recon': g_recon,\n",
    "        'g_kl_low': g_kl_low,\n",
    "        'g_kl_high': g_kl_high,\n",
    "        'g_ink': g_ink,\n",
    "        'g_bg': g_bg,\n",
    "        'D': D,\n",
    "        'K': K,\n",
    "    }\n",
    "    return gates\n",
    "\n",
    "def lbo_loss(gates):\n",
    "    gate_stack = torch.stack([\n",
    "        gates['g_recon'],\n",
    "        gates['g_kl_low'],\n",
    "        gates['g_kl_high'],\n",
    "        gates['g_ink'],\n",
    "        gates['g_bg'],\n",
    "    ], dim=0)\n",
    "    m = gate_stack.min(dim=0).values\n",
    "    return -torch.log(m).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, cfg):\n",
    "    model.train()\n",
    "    totals = []\n",
    "    for step, (x, _) in enumerate(tqdm(loader, desc='train'), start=1):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        gates = bottleneck_gates(x, x_hat, mu, logvar, cfg)\n",
    "        loss = lbo_loss(gates)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        totals.append(loss.item())\n",
    "        if step % cfg['log_every'] == 0:\n",
    "            print(\n",
    "                f\"step={step} loss={loss.item():.4f} \"\n",
    "                f\"D={gates['D'].mean().item():.4f} K={gates['K'].mean().item():.4f}\"\n",
    "            )\n",
    "    return float(np.mean(totals))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, cfg):\n",
    "    model.eval()\n",
    "    totals = []\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        gates = bottleneck_gates(x, x_hat, mu, logvar, cfg)\n",
    "        totals.append(lbo_loss(gates).item())\n",
    "    return float(np.mean(totals))\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_samples(model, loader, cfg, epoch):\n",
    "    model.eval()\n",
    "    os.makedirs(cfg['sample_dir'], exist_ok=True)\n",
    "    x, _ = next(iter(loader))\n",
    "    x = x.to(device)\n",
    "    x_hat, _, _ = model(x)\n",
    "    num = cfg['num_samples']\n",
    "    x = x[:num]\n",
    "    x_hat = x_hat[:num]\n",
    "    grid = make_grid(torch.cat([x, x_hat], dim=0), nrow=num, pad_value=1.0)\n",
    "    save_image(grid, os.path.join(cfg['sample_dir'], f\"epoch_{epoch:03d}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BHiVAE(latent_dim=config['latent_dim']).to(device)\n",
    "optimizer = Muon(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, config)\n",
    "    test_loss = evaluate(model, test_loader, config)\n",
    "    if epoch % config['sample_every'] == 0:\n",
    "        save_samples(model, test_loader, config, epoch)\n",
    "    print(f'Epoch {epoch}: train LBO {train_loss:.4f} | test LBO {test_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}