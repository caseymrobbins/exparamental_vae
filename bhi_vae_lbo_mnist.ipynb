{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BHiVAE with LBO Bottleneck (MNIST)\n",
    "\n",
    "This notebook remakes the training flow from scratch using a BHiVAE architecture with an LBO-style bottleneck and the **Muon** optimizer instead of Adam.\n",
    "\n",
    "## Colab quick start\n",
    "If you're in Colab, run the next cell to clone the repo and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Colab) Clone repo and install dependencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    repo_url = 'https://github.com/<your-org>/<your-repo>.git'  # TODO: replace\n",
    "    repo_dir = Path('/content/exparamental_vae')\n",
    "    if not repo_dir.exists():\n",
    "        !git clone {repo_url} {repo_dir}\n",
    "    %cd {repo_dir}\n",
    "    !pip -q install torch torchvision tqdm matplotlib\n",
    "else:\n",
    "    print('Not running in Colab; skipping clone/install.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'image_size': 28,\n",
    "    'latent_dim': 32,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 30,\n",
    "    'lr': 2e-3,\n",
    "    'weight_decay': 0.0,\n",
    "    'decoder_likelihood': 'bernoulli',  # 'bernoulli' -> BCE, 'gaussian' -> MSE\n",
    "    # LBO bottleneck targets\n",
    "    'log_every': 100,\n",
    "    'sample_every': 1,\n",
    "    'sample_dir': 'samples',\n",
    "    'num_samples': 8,\n",
    "    'recon_target': 0.0,\n",
    "    'recon_fail': 0.2,\n",
    "    'kl_target': 0.0,\n",
    "    'kl_fail': 50.0,\n",
    "    'squeeze_start': 8,\n",
    "    'squeeze_rate': 0.10,\n",
    "    'squeeze_threshold': 0.50,\n",
    "    'squeeze_window': 3,\n",
    "}\n",
    "config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (BHiVAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BHiVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 2, 1), nn.GroupNorm(8, 32), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.GroupNorm(8, 64), nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.GroupNorm(8, 32), nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1), nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc(x).view(x.size(0), -1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_dec(z).view(-1, 64, 7, 7)\n",
    "        return self.dec(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muon Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"Minimal Muon optimizer (momentum + normalized update direction).\n",
    "\n",
    "    This is a light-weight placeholder implementation. Adjust per your Muon spec.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=2e-4, momentum=0.95, weight_decay=1e-4):\n",
    "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    grad = grad.add(p, alpha=weight_decay)\n",
    "\n",
    "                state = self.state[p]\n",
    "                if 'velocity' not in state:\n",
    "                    state['velocity'] = torch.zeros_like(p)\n",
    "\n",
    "                v = state['velocity']\n",
    "                v.mul_(momentum).add_(grad)\n",
    "\n",
    "                denom = v.norm().clamp_min(1e-8)\n",
    "                step = v / denom\n",
    "                p.add_(step, alpha=-lr)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config['image_size']),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "print(f'Train batches: {len(train_loader)}, Test batches: {len(test_loader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBO Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu, logvar):\n",
    "    return -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).sum(1)\n",
    "\n",
    "def recon_loss(x, x_hat, likelihood='bernoulli'):\n",
    "    if likelihood == 'bernoulli':\n",
    "        return F.binary_cross_entropy(x_hat, x, reduction='none').view(x.size(0), -1).mean(1)\n",
    "    return F.mse_loss(x_hat, x, reduction='none').view(x.size(0), -1).mean(1)\n",
    "\n",
    "def normalize_score(value, target, fail):\n",
    "    return (value - fail) / (target - fail)\n",
    "\n",
    "def bottleneck_scores(x, x_hat, mu, logvar, cfg):\n",
    "    recon_err = recon_loss(x, x_hat, cfg['decoder_likelihood'])\n",
    "    kl = kl_divergence(mu, logvar)\n",
    "    s_recon = normalize_score(recon_err, cfg['recon_target'], cfg['recon_fail'])\n",
    "    s_kl = normalize_score(kl, cfg['kl_target'], cfg['kl_fail'])\n",
    "    scores = {\n",
    "        's_recon': s_recon,\n",
    "        's_kl': s_kl,\n",
    "        'recon_err': recon_err,\n",
    "        'kl': kl,\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def lbo_loss(scores):\n",
    "    score_stack = torch.stack([scores['s_recon'], scores['s_kl']], dim=0)\n",
    "    s_min = score_stack.min(dim=0).values\n",
    "    loss = -torch.log(s_min).mean()\n",
    "    return loss, s_min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_config(cfg):\n",
    "    defaults = {\n",
    "        'recon_target': 0.0,\n",
    "        'recon_fail': 0.2,\n",
    "        'kl_target': 0.0,\n",
    "        'kl_fail': 50.0,\n",
    "        'log_every': 100,\n",
    "        'sample_every': 1,\n",
    "        'sample_dir': 'samples',\n",
    "        'num_samples': 8,\n",
    "        'squeeze_start': 8,\n",
    "        'squeeze_rate': 0.10,\n",
    "        'squeeze_threshold': 0.50,\n",
    "        'squeeze_window': 3,\n",
    "    }\n",
    "    for key, value in defaults.items():\n",
    "        cfg.setdefault(key, value)\n",
    "    return cfg\n",
    "\n",
    "def train_epoch(model, loader, optimizer, cfg):\n",
    "    model.train()\n",
    "    totals = []\n",
    "    rollbacks = 0\n",
    "    for step, (x, _) in enumerate(tqdm(loader, desc='train'), start=1):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        scores = bottleneck_scores(x, x_hat, mu, logvar, cfg)\n",
    "        loss, s_min = lbo_loss(scores)\n",
    "        s_min_mean = s_min.mean()\n",
    "        if torch.isnan(s_min_mean) or (s_min_mean <= 0):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            rollbacks += 1\n",
    "            continue\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        totals.append(loss.item())\n",
    "        if step % cfg['log_every'] == 0:\n",
    "            print(\n",
    "                f\"step={step} loss={loss.item():.4f} \"\n",
    "                f\"s_min={s_min_mean.item():.4f} \"\n",
    "                f\"recon={scores['recon_err'].mean().item():.4f} \"\n",
    "                f\"kl={scores['kl'].mean().item():.4f} rollbacks={rollbacks}\"\n",
    "            )\n",
    "    avg_loss = float(np.mean(totals)) if totals else float('nan')\n",
    "    return avg_loss, rollbacks\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, cfg):\n",
    "    model.eval()\n",
    "    totals = []\n",
    "    smins = []\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        scores = bottleneck_scores(x, x_hat, mu, logvar, cfg)\n",
    "        loss, s_min = lbo_loss(scores)\n",
    "        totals.append(loss.item())\n",
    "        smins.append(s_min.mean().item())\n",
    "    return float(np.mean(totals)), float(np.mean(smins))\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_samples(model, loader, cfg, epoch):\n",
    "    model.eval()\n",
    "    os.makedirs(cfg['sample_dir'], exist_ok=True)\n",
    "    x, _ = next(iter(loader))\n",
    "    x = x.to(device)\n",
    "    x_hat, _, _ = model(x)\n",
    "    num = cfg['num_samples']\n",
    "    x = x[:num]\n",
    "    x_hat = x_hat[:num]\n",
    "    grid = make_grid(torch.cat([x, x_hat], dim=0), nrow=num, pad_value=1.0)\n",
    "    save_image(grid, os.path.join(cfg['sample_dir'], f\"epoch_{epoch:03d}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ensure_config(config)\n",
    "model = BHiVAE(latent_dim=config['latent_dim']).to(device)\n",
    "optimizer = Muon(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "smin_history = []\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    train_loss, rollbacks = train_epoch(model, train_loader, optimizer, config)\n",
    "    test_loss, test_smin = evaluate(model, test_loader, config)\n",
    "    smin_history.append(test_smin)\n",
    "    if epoch % config['sample_every'] == 0:\n",
    "        save_samples(model, test_loader, config, epoch)\n",
    "    if (\n",
    "        epoch >= config['squeeze_start']\n",
    "        and len(smin_history) >= config['squeeze_window']\n",
    "        and np.mean(smin_history[-config['squeeze_window']:]) > config['squeeze_threshold']\n",
    "    ):\n",
    "        config['recon_fail'] += config['squeeze_rate'] * (config['recon_target'] - config['recon_fail'])\n",
    "        config['kl_fail'] += config['squeeze_rate'] * (config['kl_target'] - config['kl_fail'])\n",
    "    print(\n",
    "        f\"Epoch {epoch}: train LBO {train_loss:.4f} | test LBO {test_loss:.4f} \"\n",
    "        f\"test_smin={test_smin:.4f} rollbacks={rollbacks} \"\n",
    "        f\"recon_fail={config['recon_fail']:.4f} kl_fail={config['kl_fail']:.2f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}